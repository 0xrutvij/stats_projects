---
title: "Mini Project 6"
subtitle: "CS6313.001"
author: "Rutvij Shah (rds190000)"
date: "`r format(Sys.time(), '%d %B %Y')`"
header-includes:
- \usepackage{amsmath}
- \usepackage{nicefrac}
output:
  pdf_document:
    latex_engine: xelatex
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, collapse=F, comment="##")
```

# Question 1
## Part (a)
```{r}
library(modeest)
cancer_data = read.csv("~/Downloads/prostate_cancer.csv")
attach(cancer_data)
# Investigating distribution of response variable
boxplot(psa, main="Boxplot of PSA values", ylab="PSA level (mg/ml)")

# Large number of outliers suggest the need for a transformation to prevent
# data loss
boxplot(log(psa), main="Boxplot for log of PSA values", ylab="Log PSA level")

# This suggests that log transformation does a fair job for our purposes, thus
response_var <- log(psa)

# calculating central tendencies for indicator variables
mean_cancervol = mean(cancervol)
mean_weight = mean(weight)
mean_age = mean(age)
mean_benpros = mean(benpros)
mean_capspen = mean(capspen)
mean_gleason = mean(gleason)
mod_vesinv = mfv(vesinv)
```


```{r}
# Checking the linear relation of each indicator with the response

#1 cancervol
cor(response_var, cancervol)

#2 weight
cor(response_var, weight)

#3 age
cor(response_var, age)

#4 benpros
cor(response_var, benpros)

#5 capspen
cor(response_var, capspen)

#6 gleason
cor(response_var, gleason)

#7 vesinv
# since vesinv is a qualitative var, we will use as.factor()
cor(response_var, vesinv)
```
We see that PSA's highest linear correlations are with cancervol, vesinv, gleason and capspen (in that order).

Thus, we will gradually add each to the linear model and determine its significance using ANOVA.

```{r}
#1 
f1 <- lm(response_var ~ cancervol)
summary(f1)
```
F1 is statistically significant since p-val << 0.01. Next we will add vesinv.

```{r}
#2
f2 <- lm(response_var ~ cancervol + as.factor(vesinv))
summary(f2)
```
Adding `vesinv` certainly improves the R-Squared value and reduces the residual
standard error thus we will build on F2.

```{r}
#3
f3 <- lm(response_var ~ cancervol + as.factor(vesinv) + gleason)
summary(f3)
```
Further improvements in R-Squared values couples with a drop of residual standard
error evidence that "gleason" does improve our prediction.


```{r}
f4 <- update(f3, . ~ . + capspen)
summary(f4)
```
We reject f4 since it has a negative effect on both R-Squared values and on 
residual standard errors. Also, the p-value >> 0.05. We will resume with f3

```{r}
f5 <- update(f3, . ~ . + benpros)
summary(f5)
```
R-Squared has improved from f3, residual error has further reduced and the p-val
for benpros is <<0.01 suggesting it has a significant influence on the model.
Thus we will continue with f5.

```{r}
f6 <- update(f5, . ~ . + age)
summary(f6)
f7 <- update(f5, . ~ . + weight)
summary(f7)
```
The p-values for both age & weight are >>0.05 and hence are rejected.

Thus f5 seems to be our "reasonably good" linear model for predicting PSA level
based on the data available. 

Lastly, we will check the assumptions of our model.

### (1) Are the residuals randomly distributed?
```{r}
plot(fitted(f5), resid(f5))
abline(h=0)

plot(fitted(f5), abs(resid(f5)))
```
Yes, the residues are fairly random in their distribution and hence a linear
regression model's use can be justified.

### (2) Are the residues approximately normally distributed? (Yes!)

```{r}
qqnorm(resid(f5))
qqline(resid(f5))
```

## Part (b)

Prediction of PSA value for means of indicator (or mode if qualitative).

```{r}
c1 <- c(mean_cancervol)
c2 <- c(mod_vesinv)
c3 <- c(mean_gleason)
c4 <- c(mean_benpros)
a_row_df = data.frame(c1, c2, c3, c4)
names(a_row_df) <- c("cancervol", "vesinv", "gleason", "benpros")
log_psa = predict.lm(f5, a_row_df)
pred_psa = exp(log_psa)
cat(paste("The predicted value of PSA is = ", round(pred_psa, 4)))
```

## Part (c)

Verification with model selection using backward elimination algorithm


```{r}

full = lm(response_var ~ gleason 
       + capspen 
       + benpros 
       + age
       + weight
       + cancervol
       + as.factor(vesinv))

null = lm(response_var ~ 1)

f5.auto.back_elm <- step( full, scope=list( lower=null, upper=full ), direction="backward" )

auto.log_psa = predict.lm(f5.auto.back_elm, a_row_df)
auto.pred_psa = exp(auto.log_psa)

cat(paste("\n\nThe auto predicted value of PSA is = ", round(auto.pred_psa, 4)))
```
We've verified that the most reasonable models produced are the same with both the manual and automated methods. And the predicted value generated by both attests to the fact along with the formula for linear regression.

```{r eval=FALSE}
library(equatiomatic)
extract_eq(f5, use_coefs = T)
```

### Thus, the final model is encapsulated by
$$
\operatorname{\widehat{log(psa)}} = -0.65 + 0.06(\operatorname{cancervol}) + 0.68(\operatorname{as.factor(vesinv)}_{\operatorname{1}}) + 0.33(\operatorname{gleason}) + 0.09(\operatorname{benpros})
$$
